# In [16]:
# Import the necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

import warnings
warnings.filterwarnings("ignore")

# In [17]:
# Read the CSV files
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# In [18]:
# Explore the data
print("Train Dataset Info:")
print(train_df.info())
print("\nTrain Dataset Description:")
print(train_df.describe())

print("\nCorrelation Matrix:")
plt.figure(figsize=(12, 8))
sns.heatmap(train_df.corr(), annot=True, cmap='coolwarm')
plt.show()

# In [19]:
# 1. Check unique values and types
for col in train_df.columns:
    print(f"{col}: {train_df[col].nunique()} unique values")

# 2. Handling missing values
# We'll fill missing numerical values with the median and categorical with the most frequent value
num_cols = train_df.select_dtypes(include=[np.number]).columns
cat_cols = train_df.select_dtypes(include=['object']).columns

imputer_num = SimpleImputer(strategy='median')
imputer_cat = SimpleImputer(strategy='most_frequent')

train_df[num_cols] = imputer_num.fit_transform(train_df[num_cols])
test_df[num_cols] = imputer_num.transform(test_df[num_cols])

train_df[cat_cols] = imputer_cat.fit_transform(train_df[cat_cols])
test_df[cat_cols] = imputer_cat.transform(test_df[cat_cols])

# 3. Outlier detection and treatment (optional: here using IQR method)
for col in ['Menu Price', 'Marketing Spend', 'Average Customer Spending']:
    Q1 = train_df[col].quantile(0.25)
    Q3 = train_df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    train_df[col] = np.where(train_df[col] > upper, upper, train_df[col])
    train_df[col] = np.where(train_df[col] < lower, lower, train_df[col])

# 4. Categorical encoding (Label Encoding or One-hot)
label_encoders = {}
for col in cat_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col])
    test_df[col] = le.transform(test_df[col])
    label_encoders[col] = le

# 5. Feature scaling
scaler = StandardScaler()
X_train_full = train_df.drop(columns=['Age'])  # All except target
Y_train = train_df['Age']
X_train = scaler.fit_transform(X_train_full)
X_test = scaler.transform(test_df)

# 6. Model training
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, Y_train)

# 7. Predictions
predicted_ages = model.predict(X_test)

# 8. Store predictions in test dataset
test_df['Age'] = predicted_ages

# 9. Evaluation on training data (optional)
train_pred = model.predict(X_train)
print("Train RMSE:", np.sqrt(mean_squared_error(Y_train, train_pred)))
print("Train RÂ² Score:", r2_score(Y_train, train_pred))

# 10. Final output
@evaluate
def predictions():
    '''
    Predicts the Age values for the test dataset using trained model.
    Returns the test DataFrame with the predicted Age column.
    '''
    return test_df




DDL(DATA DEFINITION LANGUAGE)
1. CREATE
	CREATE TABLE TABLENAME (COLUMNAME DATATYPE (SIZE), .....) VALUES (12, 'HI', 07/02/2002);
2. ALTER
	ALTER TABLE TABLENAME ADD(COLUMNNAME DATATYPE (SIZE))
	ALTER TABLE TABLENAME ADD(COLUMNNAME DATATYPE (SIZE), (COLUMNNAME DATATYPE (SIZE) ..........)
	ALTER TABLE TABLENAME MODIFY(COLUMNNAME DATATYPE (SIZE))
3. DELETE A COLUMN
	ALTER TABLE TABLENAME DROP COLUMN COLUMNNAME;
	ALTER TABLE TABLENAME DROP COLUMN COLUMNNAME1, COLUMNANAME2.....;
4. TRUNCATE
	TRUNCATE TABLE TABLENAME
5. DROP
	DROP TABLE TABLENAME
6. RENAME
	RENAME TABLE OLDNAME TO NEWNAME  //FOR TABLE
	ALTER TABLE TABLENAME RENAME COLUMN OLDNAME TO NEWNAME



DML(DATA MANIPULATION LANGUAGE)

1. INSERT 
	INSERT INTO TABLE NAME (COLUMNAME, ...) VALUES (VALUE1...)
2. UPDATE
	UPDATE TABLE TABLENAME SET COLUMNNAME="VALUE" WHERE (CONDN)
3. DELETE
	DELETE FROM TABLENAME
	DELETE FROM TABLENAME WHERE CONDN


1. INNER JOIN
ğŸ‘‰ Returns only the rows where there is a match in both tables.
sql
SELECT Employees.emp_id, Employees.emp_name, Departments.dept_name
FROM Employees
INNER JOIN Departments
ON Employees.dept_id = Departments.dept_id;

2. LEFT JOIN (LEFT OUTER JOIN)
ğŸ‘‰ Returns all rows from the LEFT table, even if there is no match in the RIGHT table. Missing matches
will show NULL.
sql
SELECT Employees.emp_id, Employees.emp_name, Departments.dept_name
FROM Employees
LEFT JOIN Departments
ON Employees.dept_id = Departments.dept_id;

3.  RIGHT JOIN (RIGHT OUTER JOIN)
ğŸ‘‰ Opposite of LEFT JOIN. Returns all rows from the RIGHT table, even if thereâ€™s no match in the LEFT.
sql
SELECT Employees.emp_id, Employees.emp_name, Departments.dept_name
FROM Employees
RIGHT JOIN Departments
ON Employees.dept_id = Departments.dept_id;

4. FULL JOIN (FULL OUTER JOIN)
ğŸ‘‰ Returns all rows from both tables. If no match, returns NULL.
sql
SELECT Employees.emp_id, Employees.emp_name, Departments.dept_name
FROM Employees
FULL OUTER JOIN Departments
ON Employees.dept_id = Departments.dept_id;

5. CROSS JOIN
ğŸ‘‰ Returns all possible combinations (Cartesian product).
sql
SELECT Employees.emp_id, Employees.emp_name, Departments.dept_name
FROM Employees
CROSS JOIN Departments;




1 -- INNER JOIN
SELECT columns
FROM table1
INNER JOIN table2
ON table1.column = table2.column;

2 -- LEFT JOIN / LEFT OUTER JOIN
SELECT columns
FROM table1
LEFT JOIN table2
ON table1.column = table2.column;

3 -- RIGHT JOIN / RIGHT OUTER JOIN
SELECT columns
FROM table1
RIGHT JOIN table2
ON table1.column = table2.column;

4 -- FULL OUTER JOIN
SELECT columns
FROM table1
FULL OUTER JOIN table2
ON table1.column = table2.column;

5 -- CROSS JOIN
SELECT columns
FROM table1
CROSS JOIN table2;
